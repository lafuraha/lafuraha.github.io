---
layout: post
tags: 论文/元学习
title: MAML 论文总结
math: true
date: 2024-07-20 14:47 +0800
toc:  true
category: work
---
Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks  阅读笔记  


# Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks  

![](/assets/img/MAML/IMG_MAML_20240812232829.png)  

## 研究问题

研究如何构建能通过少量新样本迅速适应新任务的模型。  
## 算法动机

旨在通过训练任务 训练模型的初始参数，使其能够快速适应新任务并达到较高性能。核心思路是优化模型的初始参数，使得模型在接收到新任务的少量数据后，通过少量梯度步更新即可显著提升性能。这一动机基于以下两点：

1. 特征学习的角度：追求构建具有高度可转移性的内部表征，这样的表征能广泛适用于多数任务。通过简单微调顶层权重，即可在新任务上取得良好表现，从而实现快速适应。
    
2. 动态系统的角度：致力于寻找对任务变化敏感的模型参数，通过最大化新任务损失函数对参数的敏感度，确保模型在面临不同任务时能够快速调整，以达到最佳性能。
## 算法及推导

元学习训练目标为：
![](/assets/img/MAML/Pasted%20image%2020240720200424.png)
MAML算法为：
![](/assets/img/MAML/Pasted%20image%2020240720185315.png)
推导见李宏毅PPT。

**一阶近似**
在分类实验中提及，不清楚具体怎么实现的，总之减少了训练时间，效果和二阶差分差不多，表明MAML的大部分改进来自于目标在更新后参数值处的梯度，而不是通过梯度更新从差分中得到的二阶更新。
<font color="#f79646">怎么近似的？</font>
## 实验

做了回归、分类和强化学习三个实验，充分验证了方法应用的广泛性。只要是基于梯度的方法，基本都能适用。
回归：不同幅值和周期的sin函数；
分类：Omniglot和MiniImagenet；
强化学习：rllab benchmark suite，没有细看。
## 后续相关工作

有人说MAML训练的不稳定性很大，需要仔细调节超参数，后续工作暂时含没有看。出现在已经读过的材料中的有： “How to train your MAML“和”(Reptile)On First-Order Meta-Learning Algorithms“。
[GitHub - dragen1860/awesome-meta-learning: A curated list of Meta-Learning resources/papers.](https://github.com/dragen1860/awesome-meta-learning)
